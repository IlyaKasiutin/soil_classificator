{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4980776,"sourceType":"datasetVersion","datasetId":2888712}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport time\nimport shutil\nimport zipfile\nimport urllib.request\nimport copy\nimport numpy as np\nimport random\nimport scipy\nfrom PIL import Image\nfrom os import listdir\nfrom os.path import isfile, join\nfrom random import randrange\nimport matplotlib.pyplot as plt\nimport glob\nfrom pandas.core.common import flatten","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-18T12:03:03.388412Z","iopub.execute_input":"2023-11-18T12:03:03.389111Z","iopub.status.idle":"2023-11-18T12:03:03.746665Z","shell.execute_reply.started":"2023-11-18T12:03:03.389076Z","shell.execute_reply":"2023-11-18T12:03:03.745846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:03:03.748335Z","iopub.execute_input":"2023-11-18T12:03:03.748872Z","iopub.status.idle":"2023-11-18T12:03:07.280929Z","shell.execute_reply.started":"2023-11-18T12:03:03.748836Z","shell.execute_reply":"2023-11-18T12:03:07.279934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_path = '../input/soil-image-dataset/Dataset/Train'\ntest_data_path = '../input/soil-image-dataset/Dataset/test'\nimg_size = (256, 256)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:03:07.302718Z","iopub.execute_input":"2023-11-18T12:03:07.303005Z","iopub.status.idle":"2023-11-18T12:03:07.308466Z","shell.execute_reply.started":"2023-11-18T12:03:07.302982Z","shell.execute_reply":"2023-11-18T12:03:07.307615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_paths = []\nclasses = []\n\nfor data_path in glob.glob(train_data_path + '/*'):\n    classes.append(data_path.split('/')[-1]) \n    train_image_paths.append(glob.glob(data_path + '/*'))\n    \ntrain_image_paths = list(flatten(train_image_paths))\nrandom.shuffle(train_image_paths)\n\nprint('train_image_path example: ', train_image_paths[0])\nprint('class example: ', classes[0])\n\n\ntrain_image_paths, valid_image_paths = train_image_paths[:int(0.8*len(train_image_paths))], train_image_paths[int(0.8*len(train_image_paths)):] \n\ntest_image_paths = []\nfor data_path in glob.glob(test_data_path + '/*'):\n    test_image_paths.append(glob.glob(data_path + '/*'))\n\ntest_image_paths = list(flatten(test_image_paths))\n\nprint(\"Train size: {}\\nValid size: {}\\nTest size: {}\".format(len(train_image_paths), len(valid_image_paths), len(test_image_paths)))","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:03:07.309549Z","iopub.execute_input":"2023-11-18T12:03:07.309879Z","iopub.status.idle":"2023-11-18T12:03:07.926675Z","shell.execute_reply.started":"2023-11-18T12:03:07.309843Z","shell.execute_reply":"2023-11-18T12:03:07.925813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_to_class = {i:j for i, j in enumerate(classes)}\nclass_to_idx = {value:key for key,value in idx_to_class.items()}","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:03:07.927895Z","iopub.execute_input":"2023-11-18T12:03:07.928241Z","iopub.status.idle":"2023-11-18T12:03:07.932936Z","shell.execute_reply.started":"2023-11-18T12:03:07.928207Z","shell.execute_reply":"2023-11-18T12:03:07.931933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-11-18T12:03:07.934354Z","iopub.execute_input":"2023-11-18T12:03:07.934701Z","iopub.status.idle":"2023-11-18T12:03:07.943949Z","shell.execute_reply.started":"2023-11-18T12:03:07.934670Z","shell.execute_reply":"2023-11-18T12:03:07.943115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddGaussianNoise:\n    def __init__(self, mean=0., stddev=.1):\n        self.mean = mean\n        self.stddev = stddev\n\n    def __call__(self, tensor):\n        noise = torch.zeros_like(tensor).normal_(self.mean, self.stddev)\n        return tensor.add_(noise)\n\n\ntrain_transformation = transforms.Compose([\n    transforms.Resize(size=img_size),\n    transforms.ToTensor(),\n#     AddGaussianNoise(),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.3),\n])\n\ntest_transformation = transforms.Compose([\n        transforms.Resize(size=img_size),\n        transforms.ToTensor(),\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:15:54.927272Z","iopub.execute_input":"2023-11-18T15:15:54.927638Z","iopub.status.idle":"2023-11-18T15:15:54.935834Z","shell.execute_reply.started":"2023-11-18T15:15:54.927608Z","shell.execute_reply":"2023-11-18T15:15:54.934827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LandmarkDataset(Dataset):\n    def __init__(self, image_paths, transform=False):\n        self.image_paths = image_paths\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_paths[idx]\n        image = Image.open(image_filepath)\n        image = image.convert('RGB')\n        \n        label = image_filepath.split('/')[-2]\n        label = class_to_idx[label]\n        if self.transform is not None:\n            try:\n                image = self.transform(image)\n            except RuntimeError as e:\n                print(image_filepath)\n                print(type(image))\n        \n        return image, label\n    \n\ntrain_dataset = LandmarkDataset(train_image_paths,train_transformation)\nvalid_dataset = LandmarkDataset(valid_image_paths,train_transformation) #test transforms are applied\ntest_dataset = LandmarkDataset(test_image_paths,test_transformation)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:15:55.276856Z","iopub.execute_input":"2023-11-18T15:15:55.277250Z","iopub.status.idle":"2023-11-18T15:15:55.285653Z","shell.execute_reply.started":"2023-11-18T15:15:55.277220Z","shell.execute_reply":"2023-11-18T15:15:55.284607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The shape of tensor for 50th image in train dataset: ',train_dataset[49][0].shape)\nprint('The label for 50th image in train dataset: ',train_dataset[49][1])","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:15:55.568792Z","iopub.execute_input":"2023-11-18T15:15:55.569130Z","iopub.status.idle":"2023-11-18T15:15:55.590804Z","shell.execute_reply.started":"2023-11-18T15:15:55.569102Z","shell.execute_reply":"2023-11-18T15:15:55.589935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = test_dataset[1][0]\nplt.imshow(transforms.ToPILImage()(image), interpolation=\"bicubic\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:16:29.391440Z","iopub.execute_input":"2023-11-18T15:16:29.392359Z","iopub.status.idle":"2023-11-18T15:16:29.812741Z","shell.execute_reply.started":"2023-11-18T15:16:29.392322Z","shell.execute_reply":"2023-11-18T15:16:29.811700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset, batch_size=8, shuffle=True\n)\n\nval_loader = DataLoader(\n    valid_dataset, batch_size=8, shuffle=True\n)\n\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=8, shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:16:29.814248Z","iopub.execute_input":"2023-11-18T15:16:29.814557Z","iopub.status.idle":"2023-11-18T15:16:29.820104Z","shell.execute_reply.started":"2023-11-18T15:16:29.814530Z","shell.execute_reply":"2023-11-18T15:16:29.819064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\nfrom torchvision.models import mobilenet_v3_small\n\nmodel = mobilenet_v3_small(weights=None)\nmodel.classifier[3] = nn.Linear(in_features=1024, out_features=64, bias=True)\nmodel.classifier.append(nn.Linear(in_features=64, out_features=4, bias=True))\nmodel.classifier","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:17:50.417279Z","iopub.execute_input":"2023-11-18T15:17:50.417683Z","iopub.status.idle":"2023-11-18T15:17:50.489547Z","shell.execute_reply.started":"2023-11-18T15:17:50.417650Z","shell.execute_reply":"2023-11-18T15:17:50.488512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):\n    # Set the model to training mode\n    model.train()\n    train_loss = 0\n    print(\"Epoch:\", epoch)\n    counter = 1\n    # Process the images in batches\n    for data, target in tqdm.tqdm_notebook(train_loader):\n        # Use the CPU or GPU as appropriate\n        # Recall that GPU is optimized for the operations we are dealing with\n        data, target = data.to(device), target.to(device)\n        \n        # Reset the optimizer\n        optimizer.zero_grad()\n        \n        # Push the data forward through the model layers\n        output = model(data)\n        predicted = torch.argmax(output.data, dim=1)\n\n        # Get the loss\n        loss = loss_function(output, target)\n\n        # Keep a running total\n        train_loss += loss.item()\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        # Print metrics so we see some progress\n        print('\\tLoss: {:.6f}'.format(loss.item()))\n        counter += 1\n            \n    avg_loss = train_loss / counter\n    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:17:51.020469Z","iopub.execute_input":"2023-11-18T15:17:51.020768Z","iopub.status.idle":"2023-11-18T15:17:51.028766Z","shell.execute_reply.started":"2023-11-18T15:17:51.020724Z","shell.execute_reply":"2023-11-18T15:17:51.027773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        batch_count = 0\n        for data, target in test_loader:\n            batch_count += 1\n            data, target = data.to(device), target.to(device)\n            \n            output = model(data)\n            \n            test_loss += loss_function(output, target).item()\n            \n\n            predicted = torch.argmax(output.data, dim=1)\n            print(\"VALIDATION\")\n            print(f\"target:{target[:10]}\")\n            print(f\"predicted:{predicted[:10]}\")\n            correct += torch.sum(target.data==predicted)\n\n    avg_loss = test_loss / batch_count\n    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        avg_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:17:51.191875Z","iopub.execute_input":"2023-11-18T15:17:51.192124Z","iopub.status.idle":"2023-11-18T15:17:51.200175Z","shell.execute_reply.started":"2023-11-18T15:17:51.192102Z","shell.execute_reply":"2023-11-18T15:17:51.199285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=1e-2)\n\nloss_function = nn.CrossEntropyLoss()\n\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\nepochs = 30\n\ndevice = \"cpu\"\nif (torch.cuda.is_available()):\n    device = \"cuda\"\n    model.cuda()\n\nprint('Training on', device)\nfor epoch in range(1, epochs + 1):\n    train_loss = train(model, device, train_loader, optimizer, epoch)\n    val_loss = test(model, device, val_loader)\n    epoch_nums.append(epoch)\n    training_loss.append(train_loss)\n    validation_loss.append(val_loss)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:17:53.136867Z","iopub.execute_input":"2023-11-18T15:17:53.137253Z","iopub.status.idle":"2023-11-18T15:25:09.821739Z","shell.execute_reply.started":"2023-11-18T15:17:53.137204Z","shell.execute_reply":"2023-11-18T15:25:09.820771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:27:52.583580Z","iopub.execute_input":"2023-11-18T15:27:52.584476Z","iopub.status.idle":"2023-11-18T15:27:52.868064Z","shell.execute_reply.started":"2023-11-18T15:27:52.584442Z","shell.execute_reply":"2023-11-18T15:27:52.867146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\nimport seaborn as sns\n\n\ntruelabels = []\npredictions = []\nmodel.eval()\nprint(\"Getting predictions from test set...\")\nwith torch.no_grad():\n    for data, target in test_loader:\n        data, target = data.to(device), target.to(device)\n        output = model(data)\n        output = output.cpu()\n        predicted = torch.argmax(output.data, dim=1)\n        data = data.cpu()\n        target = target.cpu()\n        \n        for label in target.data.numpy():\n            truelabels.append(label)\n        for prediction in predicted:\n            predictions.append(prediction) \n\n    # Plot the confusion matrix\n    cm = confusion_matrix(truelabels, predictions)\n    tick_marks = np.arange(len(classes))\n\n    df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n    plt.figure(figsize = (7,7))\n    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n    plt.xlabel(\"Predicted Shape\", fontsize = 20)\n    plt.ylabel(\"True Shape\", fontsize = 20)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:44:51.234404Z","iopub.execute_input":"2023-11-18T15:44:51.235162Z","iopub.status.idle":"2023-11-18T15:44:54.115238Z","shell.execute_reply.started":"2023-11-18T15:44:51.235127Z","shell.execute_reply":"2023-11-18T15:44:54.114313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_params')","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:53:40.272366Z","iopub.execute_input":"2023-11-18T15:53:40.273345Z","iopub.status.idle":"2023-11-18T15:53:40.311262Z","shell.execute_reply.started":"2023-11-18T15:53:40.273306Z","shell.execute_reply":"2023-11-18T15:53:40.310386Z"},"trusted":true},"execution_count":null,"outputs":[]}]}